---
title: "Multi-Path Model Selection with Stability Analysis"
author: "Michael Asante Ofosu, Mohammad Al Srayheen, Soroosh Alavi"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: cosmo
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, fig.height = 6)
set.seed(2025)
```

# Introduction

This document demonstrates the **multipathaic** package for multi-path model selection with stability analysis.

## Package Overview

Traditional forward selection follows a single "best" path through model space. Our approach:

1. **Explores multiple paths** simultaneously (multi-path forward selection)
2. **Evaluates stability** via bootstrap resampling
3. **Identifies plausible models** combining AIC and stability criteria

## Installation
```{r install, eval=FALSE}
# Install from GitHub
remotes::install_github("R-4-Data-Science/Final_Project_multipathaic")
```
```{r load}
# Load package
library(multipathaic)
```

---

# Example 1: Linear Regression

## Simulate Data
```{r simulate_linear}
# Simulation parameters
n <- 100
p <- 7
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)

# True model: y = x1 + 0.5*x2 + noise
# x3-x7 are noise variables
y <- X[,1] + 0.5 * X[,2] + rnorm(n)

cat("Generated data:\n")
cat(sprintf("  n = %d observations\n", n))
cat(sprintf("  p = %d predictors\n", p))
cat(sprintf("  True model: y = x1 + 0.5*x2 + noise\n"))
```

## Step 1: Multi-Path Forward Selection
```{r paths_linear}
paths_linear <- build_paths(
  X = X,
  y = y,
  family = "gaussian",
  K = 5,           # Explore up to 5 predictors
  eps = 1e-6,      # Minimal improvement threshold
  delta = 1.5,     # Keep alternatives within 1.5 AIC
  L = 50,          # Max 50 models per step
  verbose = TRUE
)

print(paths_linear)
```

### Visualize Model Exploration
```{r aic_plot_linear, fig.width=10, fig.height=6}
plot_aic_by_step(paths_linear)
```

**Interpretation:** AIC decreases as we add predictors, with diminishing returns after 2-3 variables.

## Step 2: Stability Analysis
```{r stability_linear}
stab_linear <- stability(
  X = X,
  y = y,
  family = "gaussian",
  B = 40,              # 40 bootstrap resamples
  resample_type = "bootstrap",
  K = 5,
  eps = 1e-6,
  delta = 1.5,
  L = 50,
  verbose = TRUE
)

print(stab_linear)
```

### Visualize Stability Scores
```{r stability_plot_linear, fig.width=10, fig.height=6}
plot_stability(stab_linear, threshold = 0.6)
```

**Interpretation:** Variables x1 and x2 show high stability (π > 0.8), correctly identifying the true predictors!

## Step 3: Select Plausible Models
```{r plausible_linear}
plausible_linear <- plausible_models(
  path_result = paths_linear,
  stability_result = stab_linear,
  Delta = 2,              # Within 2 AIC of best
  tau = 0.6,              # Average stability >= 0.6
  remove_duplicates = TRUE,
  refit = TRUE,
  X = X,
  y = y
)

print(plausible_linear)
```

### Display Plausible Models
```{r plausible_table_linear}
# Show key information
display_cols <- c("size", "variables", "aic", "delta_aic", "avg_stability")
knitr::kable(plausible_linear[, display_cols], 
             digits = 3,
             caption = "Plausible Models for Linear Regression")
```

**Key Finding:** The plausible models include x1 and x2 (the true predictors) with high stability!

---

# Example 2: Logistic Regression

## Simulate Binary Data
```{r simulate_logistic}
# Binary outcome
n <- 120
p <- 6
X <- matrix(rnorm(n * p), n, p)
colnames(X) <- paste0("x", 1:p)

# True model: logit(P(y=1)) = 0.5*x1 - 0.3*x2
eta <- 0.5 * X[,1] - 0.3 * X[,2]
prob <- 1 / (1 + exp(-eta))
y <- rbinom(n, 1, prob)

cat("Generated binary data:\n")
cat(sprintf("  n = %d observations\n", n))
cat(sprintf("  p = %d predictors\n", p))
cat(sprintf("  Outcome: %.1f%% positive class\n", 100 * mean(y)))
```

## Run Full Pipeline
```{r logistic_pipeline}
# Multi-path search
paths_logistic <- build_paths(
  X, y, 
  family = "binomial",
  K = 4,
  delta = 1.5,
  verbose = FALSE
)

# Stability analysis
stab_logistic <- stability(
  X, y,
  family = "binomial",
  B = 30,
  K = 4,
  delta = 1.5,
  verbose = FALSE
)

# Plausible models
plausible_logistic <- plausible_models(
  paths_logistic,
  stab_logistic,
  Delta = 2,
  tau = 0.5,
  refit = TRUE,
  X = X,
  y = y
)

print(plausible_logistic)
```

## Evaluate Classification Performance
```{r confusion_logistic}
# Get best model
best_model <- plausible_logistic$fitted_model[[1]]

# Compute metrics
metrics <- confusion_metrics(best_model, X, y, threshold = 0.5)

# Display confusion matrix
cat("\nConfusion Matrix:\n")
print(metrics$confusion_matrix)

cat("\nPerformance Metrics:\n")
print(metrics$metrics)
```

## Visualizations
```{r viz_logistic, fig.width=10, fig.height=6}
# Stability scores
plot_stability(stab_logistic, threshold = 0.5)
```

**Interpretation:** x1 and x2 show highest stability, matching the true model!

---

# Advanced Visualizations

## Model Path Tree
```{r model_tree_linear, fig.width=12, fig.height=7}
plot_model_tree(paths_linear, max_models = 20, highlight_best = TRUE)
```

**Interpretation:** The tree shows branching paths explored. Red points = best models at each step.

## Variable Importance Ranking
```{r importance_linear}
importance_linear <- variable_importance_ranking(paths_linear, stab_linear)
print(importance_linear)
```
```{r importance_plot_linear, fig.width=10, fig.height=6}
plot_variable_importance(importance_linear, top_n = 7)
```

## Variable Co-occurrence Heatmap
```{r heatmap_linear, fig.width=9, fig.height=8}
if (nrow(plausible_linear) > 1) {
  plot_variable_heatmap(plausible_linear)
}
```

## Bootstrap Stability Distribution
```{r stability_dist_linear, fig.width=10, fig.height=6}
plot_stability_distribution(stab_linear, top_n = 5)
```

## Comprehensive Dashboard
```{r dashboard_linear, fig.width=12, fig.height=10}
plot_model_dashboard(paths_linear, stab_linear, plausible_linear)
```

---

# Summary

## Key Results

### Linear Regression:
- Identified **`r nrow(plausible_linear)` plausible models**
- Variables x1 and x2 (true predictors) show **π > 0.8**
- All plausible models include x1 and x2

### Logistic Regression:
- Identified **`r nrow(plausible_logistic)` plausible models**
- Best model achieves **`r sprintf("%.1f%%", 100 * metrics$metrics$Value[1])` accuracy**
- Stability correctly identifies important predictors

## Advantages of This Approach

1. **Explores alternatives:** Doesn't commit to single "best" path
2. **Quantifies reliability:** Stability scores reveal robust predictors
3. **Transparent:** Provides set of plausible models, not false confidence
4. **Reproducible:** Bootstrap-based stability is more stable than single-path selection

---

# Session Info
```{r session}
sessionInfo()
```

---

**End of Demo**

GitHub Repository: install_github("R-4-Data-Science/Final_Project_multipathaic")
